{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a5f0b7-6572-4fce-84a7-e545f7782dbd",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max Scaling is a normalization technique used in data preprocessing to rescale the features of a dataset so that they fall within a specified range, usually [0, 1] or [-1, 1]. This technique preserves the relationships between the original data points by maintaining the relative distances.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "$\n",
    "X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\times (\\text{new max} - \\text{new min}) + \\text{new min}\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X$ is the original value.\n",
    "- $X_{\\text{min}}$ and $X_{\\text{max}}$ are the minimum and maximum values in the dataset.\n",
    "- $\\text{new min}$ and $\\text{new max}$ are the desired minimum and maximum values for the scaled data.\n",
    "\n",
    "**Example**: Consider a dataset of house prices in thousands of dollars: [100, 200, 300, 400, 500]. To scale these values to a range of [0, 1], we would apply the Min-Max scaling formula:\n",
    "\n",
    "1. Find the minimum and maximum values: $X_{\\text{min}} = 100$, $X_{\\text{max}} = 500$.\n",
    "2. Apply the formula to each value. For example, for $X = 200$:\n",
    "\n",
    "$\n",
    "X' = \\frac{200 - 100}{500 - 100} \\times (1 - 0) + 0 = \\frac{100}{400} = 0.25\n",
    "$\n",
    "\n",
    "Applying the formula to all values would give a scaled dataset of [0, 0.25, 0.5, 0.75, 1].\n",
    "\n",
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "Unit Vector Scaling (or normalization to unit length) scales the feature vector to have a unit norm (length of 1). This is often used when the direction of the vector is more important than its magnitude.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$\n",
    "X' = \\frac{X}{\\|X\\|}\n",
    "$\n",
    "\n",
    "where $\\|X\\|$ is the Euclidean norm of the vector $X$.\n",
    "\n",
    "**Difference from Min-Max Scaling:**\n",
    "\n",
    "- **Unit Vector Scaling** transforms the data based on its direction, ensuring the magnitude of the vector is 1. This is useful for distance-based algorithms like k-NN or SVM.\n",
    "- **Min-Max Scaling** rescales each feature individually to a specific range, which is helpful for algorithms that rely on data within a standard range.\n",
    "\n",
    "**Example**: Consider a dataset with a feature vector $[3, 4]$. The Euclidean norm of the vector is:\n",
    "\n",
    "$\n",
    "\\|X\\| = \\sqrt{3^2 + 4^2} = 5\n",
    "$\n",
    "\n",
    "The scaled vector using the unit vector technique is:\n",
    "\n",
    "$\n",
    "X' = \\left[ \\frac{3}{5}, \\frac{4}{5} \\right] = [0.6, 0.8]\n",
    "$\n",
    "\n",
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a high-dimensional dataset into a lower-dimensional space while retaining as much variance as possible. PCA identifies the directions (principal components) along which the data varies the most and projects the data onto these directions.\n",
    "\n",
    "**Application of PCA:**\n",
    "\n",
    "1. **Standardize the Data**: Ensure each feature has a mean of 0 and standard deviation of 1.\n",
    "2. **Compute the Covariance Matrix**: Find the covariance matrix of the data.\n",
    "3. **Compute Eigenvalues and Eigenvectors**: Determine the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Select Principal Components**: Choose the top $k$ eigenvectors corresponding to the largest eigenvalues.\n",
    "5. **Transform the Data**: Project the original data onto the selected principal components.\n",
    "\n",
    "**Example**: Consider a dataset with two features (X, Y). After applying PCA, you might find that 95% of the variance is captured by the first principal component, and 5% by the second. You could reduce the dimensionality by projecting the data onto the first principal component, simplifying the dataset while retaining most of the information.\n",
    "\n",
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "PCA can be used for Feature Extraction by transforming the original features into a new set of uncorrelated features called principal components. These components capture the maximum variance in the data and represent the most significant patterns or directions of variability.\n",
    "\n",
    "**Example of PCA for Feature Extraction:**\n",
    "\n",
    "Suppose you have a dataset with 5 features (A, B, C, D, E). After applying PCA, you find that two principal components capture 90% of the variance. You can then use these two components as new features, effectively reducing the feature set from 5 to 2 while retaining most of the important information.\n",
    "\n",
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "To build a recommendation system for a food delivery service, you have features such as price, rating, and delivery time. Here's how you would use Min-Max scaling:\n",
    "\n",
    "1. **Normalize each feature**: For each feature (price, rating, delivery time), compute the minimum and maximum values.\n",
    "2. **Apply Min-Max Scaling**: Scale each feature to a range of [0, 1] or [-1, 1] depending on the algorithmâ€™s requirements.\n",
    "   - For price, convert all values to a scale where the lowest price is 0, and the highest is 1.\n",
    "   - Similarly, apply the same transformation for rating and delivery time.\n",
    "3. **Ensure compatibility across features**: This allows the recommendation model to treat all features equally, without one feature disproportionately affecting the results due to differing scales.\n",
    "\n",
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "To reduce the dimensionality of a dataset for predicting stock prices:\n",
    "\n",
    "1. **Standardize the Data**: Ensure that all features (e.g., financial data, market trends) are on the same scale.\n",
    "2. **Apply PCA**:\n",
    "   - Compute the covariance matrix of the features.\n",
    "   - Determine the principal components and choose the top $k$ components that capture most of the variance (e.g., 95%).\n",
    "   - Transform the Data: Use these $k$ components as new features, reducing dimensionality while maintaining important information.\n",
    "3. **Build the Model**: Train your stock price prediction model using the reduced dataset.\n",
    "\n",
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "Given the dataset [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of [-1, 1].\n",
    "\n",
    "1. Find the minimum and maximum values: $X_{\\text{min}} = 1$, $X_{\\text{max}} = 20$.\n",
    "2. Apply the Min-Max formula for a range of [-1, 1]:\n",
    "\n",
    "$\n",
    "X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\times (1 - (-1)) + (-1)\n",
    "$\n",
    "\n",
    "For $X = 5$:\n",
    "\n",
    "$\n",
    "X' = \\frac{5 - 1}{20 - 1} \\times 2 - 1 = \\frac{4}{19} \\times 2 - 1 = \\frac{8}{19} - 1 \\approx -0.58\n",
    "$\n",
    "\n",
    "Repeat for other values to get the transformed dataset.\n",
    "\n",
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To perform Feature Extraction using PCA:\n",
    "\n",
    "1. **Standardize the Data**: Convert all features to a similar scale.\n",
    "2. **Apply PCA**: Calculate the covariance matrix, eigenvalues, and eigenvectors.\n",
    "3. **Determine Principal Components**: Select the number of principal components that capture most of the variance (e.g., 95%).\n",
    "\n",
    "**Choosing the Number of Principal Components:**\n",
    "\n",
    "- Use techniques like the Scree Plot or Explained Variance to decide the number of components. Generally, you retain enough components to explain a high percentage (e.g., 95%) of the variance.\n",
    "- If two principal components capture most of the variance, you might reduce the dataset from 5 to 2 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fd520c-83c5-4b37-918d-02a54ade540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.57894737, -0.05263158,  0.47368421,  1.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7. CODE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the desired range for Min-Max scaling\n",
    "min_new, max_new = -1, 1\n",
    "\n",
    "# Calculate the min and max of the original dataset\n",
    "min_old, max_old = data.min(), data.max()\n",
    "\n",
    "# Apply Min-Max scaling formula\n",
    "scaled_data = (data - min_old) * (max_new - min_new) / (max_old - min_old) + min_new\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e6dfaa-f3c7-49fd-a167-14ac29e5a288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.7079117 , 0.1960794 , 0.06672959, 0.02094925, 0.00833005]),\n",
       " array([0.7079117 , 0.9039911 , 0.9707207 , 0.99166995, 1.        ]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8. CODE\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset: [height, weight, age, gender, blood pressure]\n",
    "# Hypothetical data for 10 samples\n",
    "data = np.array([\n",
    "    [170, 65, 30, 1, 120],  # Sample 1\n",
    "    [160, 70, 25, 0, 130],  # Sample 2\n",
    "    [180, 80, 35, 1, 140],  # Sample 3\n",
    "    [175, 75, 40, 0, 150],  # Sample 4\n",
    "    [165, 68, 29, 1, 125],  # Sample 5\n",
    "    [155, 55, 23, 0, 115],  # Sample 6\n",
    "    [185, 90, 45, 1, 160],  # Sample 7\n",
    "    [158, 60, 33, 0, 135],  # Sample 8\n",
    "    [172, 85, 28, 1, 145],  # Sample 9\n",
    "    [168, 72, 38, 0, 128]   # Sample 10\n",
    "])\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(data_standardized)\n",
    "\n",
    "# Get the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "explained_variance_ratio, cumulative_variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cdce5-f1aa-44b8-a0e3-5ebcf3a53b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
